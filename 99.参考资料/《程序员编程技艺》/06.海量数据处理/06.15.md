## 本章海量数据的习题

**1**
有 100W 个关键字，长度小于等于 50 字节。用高效的算法找出 top10 的热词，并对内存的占用不超过 1MB。

提示：老题，与 caopengcs 讨论后，得出具体思路为：

- 先把 100W 个关键字 hash 映射到小文件，根据题意，100W*50B = 50*10^6B = 50M，而内存只有 1M，故干脆搞一个 hash 函数 % 50，分解成 50 个小文件；
- 针对对每个小文件依次运用 hashmap(key，value)完成每个 key 的 value 次数统计，后用堆找出每个小文件中 value 次数最大的 top 10； -最后依次对每两小文件的 top 10 归并，得到最终的 top 10。

此外，很多细节需要注意下，举个例子，如若 hash 映射后导致分布不均的话，有的小文件可能会超过 1M，故为保险起见，你可能会说根据数据范围分解成 50~500 或更多的小文件，但到底是多少呢？我觉得这不重要，勿纠结答案，虽准备在平时，但关键还是看临场发挥，保持思路清晰关注细节即可。

**2**

单机 5G 内存，磁盘 200T 的数据，分别为字符串，然后给定一个字符串，判断这 200T 数据里面有没有这个字符串，怎么做？
如果查询次数会非常的多, 怎么预处理？

提示：如果数据是 200g 且允许少许误差的话，可以考虑用布隆过滤器 Bloom Filter。但本题是 200T，得另寻良策，具体解法请读者继续思考。

**3**

现在有一个大文件，文件里面的每一行都有一个 group 标识（group 很多，但是每个 group 的数据量很小），现在要求把这个大文件分成十个小文件，要求：

- 1、同一个 group 的必须在一个文件里面；
- 2、切分之后，要求十个小文件的数据量尽可能均衡。

**7**

服务器内存 1G，有一个 2G 的文件，里面每行存着一个 QQ 号（5-10 位数），怎么最快找出出现过最多次的 QQ 号。

**8**

尽量高效的统计一片英文文章（总单词数目）里出现的所有英文单词，按照在文章中首次出现的顺序打印输出该单词和它的出现次数。

**9**

在人人好友里，A 和 B 是好友，B 和 C 是好友，如果 A 　和 C 不是好友，那么 C 是 A 的二度好友，在一个有 10 万人的数据库里，如何在时间０（ｎ）里，找到某个人的十度好友。

**12**

海量记录，记录形式如下： TERMID URLNOCOUNT urlno1 urlno2 ..., urlnon，请问怎么考虑资源和时间这两个因素，实现快速查询任意两个记录的交集，并集等，设计相关的数据结构和算法。

**14**

有一亿个整数，请找出最大的 1000 个，要求时间越短越好，空间占用越少越好。

**18**

10 亿个 int 型整数，如何找出重复出现的数字。

**19**

有 2G 的一个文本文档，文件每行存储的是一个句子，每个单词是用空格隔开的。问：输入一个句子，如何找到和它最相似的前 10 个句子。

提示：可用倒排文档。

**20**

某家视频网站，每天有上亿的视频被观看，现在公司要请研发人员找出最热门的视频。
该问题的输入可以简化为一个字符串文件，每一行都表示一个视频 id，然后要找出出现次数最多的前 100 个视频 id，将其输出，同时输出该视频的出现次数。

- 1.假设每天的视频播放次数为 3 亿次，被观看的视频数量为一百万个，每个视频 ID 的长度为 20 字节，限定使用的内存为 1G。请简述做法，再写代码。
- 2.假设每个月的视频播放次数为 100 亿次，被观看的视频数量为 1 亿，每个视频 ID 的长度为 20 字节，一台机器被限定使用的内存为 1G。

提示：万变不离其宗，分而治之/Hash 映射 + Hash 统计 + 堆/快速/归并排序。

**21**

有一个 log 文件，里面记录的格式为：

    QQ号     时间        flag

    123456   14：00：00     0

    123457   14：00：01     1

其中 flag=0 表示登录 flag=1 表示退出

问：统计一天平均在线的 QQ 数。

**22**

一个文本，一万行，每行一个词，统计出现频率最高的前 10 个词（词的平均长度为 Len）。并分析时间复杂度。

**23**

在一个文件中有 10G 个整数，乱序排列，要求找出中位数。内存限制为 2G。只写出思路即可。

**24**

一个 url 指向的页面里面有另一个 url,最终有一个 url 指向之前出现过的 url 或空，这两种情形都定义为 null。这样构成一个单链表。给两条这样单链表，判断里面是否存在同样的 url。url 以亿级计，资源不足以 hash。

**25**

一个 1G 大小的一个文件，里面每一行是一个词，词的大小不超过 16 字节，内存限制大小是 1M。返回频数最高的 100 个词。

**26**

1000 万字符串，其中有些是重复的，需要把重复的全部去掉，保留没有重复的字符串。请怎么设计和实现？

**27**
有 10 个文件，每个文件 1G，每个文件的每一行都存放的是用户的 query，每个文件的 query 都可能重复。要你按照 query 的频度排序。

**28**

现有一 200M 的文本文件，里面记录着 IP 地址和对应地域信息，如

    202.100.83.56 北京 北京大学

    202.100.83.120 北京 人民大学

    202.100.83.134 北京 中国青年政治学院

    211.93.120.45 长春市 长春大学

    211.93.120.129 吉林市 吉林大学

    211.93.120.200 长春 长春KTV

现有 6 亿个 IP 地址，请编写程序，读取 IP 地址便转换成 IP 地址相对应的城市，要求有较好的时间复杂度和空间复杂度。
