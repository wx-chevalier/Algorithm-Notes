## 分而治之

### 方法介绍

对于海量数据而言，由于无法一次性装进内存处理，导致我们不得不把海量的数据通过 hash 映射分割成相应的小块数据，然后再针对各个小块数据通过 hash_map 进行统计或其它操作。

那什么是 hash 映射呢？简单来说，就是为了便于计算机在有限的内存中处理 big 数据，我们通过一种映射散列的方式让数据均匀分布在对应的内存位置(如大数据通过取余的方式映射成小数存放在内存中，或大文件映射成多个小文件)，而这个映射散列方式便是我们通常所说的 hash 函数，设计的好的 hash 函数能让数据均匀分布而减少冲突。

### 问题实例

**1、海量日志数据，提取出某日访问百度次数最多的那个 IP**

**分析**：百度作为国内第一大搜索引擎，每天访问它的 IP 数量巨大，如果想一次性把所有 IP 数据装进内存处理，则内存容量明显不够，故针对数据太大，内存受限的情况，可以把大文件转化成（取模映射）小文件，从而大而化小，逐个处理。

换言之，先映射，而后统计，最后排序。

**解法**：具体分为以下 3 个步骤

- 1.分而治之/hash 映射
- 首先把这一天访问百度日志的所有 IP 提取出来，然后逐个写入到一个大文件中，接着采用映射的方法，比如%1000，把整个大文件映射为 1000 个小文件。
- 2.hash_map 统计
- 当大文件转化成了小文件，那么我们便可以采用 hash_map(ip, value)来分别对 1000 个小文件中的 IP 进行频率统计，再找出每个小文件中出现频率最大的 IP。
- 3.堆/快速排序
- 统计出 1000 个频率最大的 IP 后，依据各自频率的大小进行排序(可采取堆排序)，找出那个频率最大的 IP，即为所求。

**注**：Hash 取模是一种等价映射，不会存在同一个元素分散到不同小文件中去的情况，即这里采用的是%1000 算法，那么同一个 IP 在 hash 后，只可能落在同一个文件中，不可能被分散的。

**2、寻找热门查询，300 万个查询字符串中统计最热门的 10 个查询**

**原题**：搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为 1-255 字节。假设目前有一千万个记录，请你统计最热门的 10 个查询串，要求使用的内存不能超过 1G。

**分析**：这些查询串的重复度比较高，虽然总数是 1 千万，但如果除去重复后，不超过 3 百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门。

由上面第 1 题，我们知道，数据大则划为小的，例如一亿个 ip 求 Top 10，可先%1000 将 ip 分到 1000 个小文件中去，并保证一种 ip 只出现在一个文件中，再对每个小文件中的 ip 进行 hash_map 统计并按数量排序，最后归并或者最小堆依次处理每个小文件的 top10 以得到最后的结果。

但对于本题，数据规模比较小，能一次性装入内存。因为根据题目描述，虽然有一千万个 Query，但是由于重复度比较高，故去除重复后，事实上只有 300 万的 Query，每个 Query255Byte，因此我们可以考虑把他们都放进内存中去（300 万个字符串假设没有重复，都是最大长度，那么最多占用内存 3M\*1K/4=0.75G。所以可以将所有字符串都存放在内存中进行处理）。

所以我们放弃分而治之/hash 映射的步骤，直接上 hash_map 统计，然后排序。So，针对此类典型的 TOP K 问题，采取的对策往往是：hash_map + 堆。

**解法**：

- 1.hash_map 统计
- 先对这批海量数据预处理。具体方法是：维护一个 Key 为 Query 字串，Value 为该 Query 出现次数的 hash_map，即 hash_map(Query, Value)，每次读取一个 Query，如果该字串不在 Table 中，那么加入该字串，并将 Value 值设为 1；如果该字串在 Table 中，那么将该字串的计数加 1 即可。最终我们在 O(N)的时间复杂度内用 hash_map 完成了统计；
- 2.堆排序
- 借助堆这个数据结构，找出 Top K，时间复杂度为 N‘logK。即借助堆结构，我们可以在 log 量级的时间内查找和调整/移动。因此，维护一个 K(该题目中是 10)大小的小根堆，然后遍历 300 万的 Query，分别和根元素进行对比。所以，我们最终的时间复杂度是：O(n) + N' \* O(logk），其中，N 为 1000 万，N’为 300 万。

关于第 2 步堆排序，可以维护 k 个元素的最小堆，即用容量为 k 的最小堆存储最先遍历到的 k 个数，并假设它们即是最大的 k 个数，建堆费时 O（k），并调整堆(费时 O(logk))后，有 k1>k2>...kmin（kmin 设为小顶堆中最小元素）。继续遍历数列，每次遍历一个元素 x，与堆顶元素比较，若 x>kmin，则更新堆（x 入堆，用时 logk），否则不更新堆。这样下来，总费时 O（k*logk+（n-k）*logk）=O（n\*logk）。此方法得益于在堆中，查找等各项操作时间复杂度均为 logk。

当然，你也可以采用 trie 树，关键字域存该查询串出现的次数，没有出现为 0。最后用 10 个元素的最小推来对出现频率进行排序。

**3、有一个 1G 大小的一个文件，里面每一行是一个词，词的大小不超过 16 字节，内存限制大小是 1M。返回频数最高的 100 个词**

**解法**：

- 1.分而治之/hash 映射
- 顺序读取文件，对于每个词 x，取 hash(x)%5000，然后把该值存到 5000 个小文件（记为 x0,x1,...x4999）中。这样每个文件大概是 200k 左右。当然，如果其中有的小文件超过了 1M 大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过 1M。
- 2.hash_map 统计
- 对每个小文件，采用 trie 树/hash_map 等统计每个文件中出现的词以及相应的频率。
- 3.堆/归并排序
- 取出出现频率最大的 100 个词（可以用含 100 个结点的最小堆）后，再把 100 个词及相应的频率存入文件，这样又得到了 5000 个文件。最后就是把这 5000 个文件进行归并（类似于归并排序）的过程了。

**4、海量数据分布在 100 台电脑中，想个办法高效统计出这批数据的 TOP10**

**解法一**：

如果同一个数据元素只出现在某一台机器中，那么可以采取以下步骤统计出现次数 TOP10 的数据元素：

- 1.堆排序
- 在每台电脑上求出 TOP 10，可以采用包含 10 个元素的堆完成（TOP 10 小，用最大堆，TOP 10 大，用最小堆，比如求 TOP10 大，我们首先取前 10 个元素调整成最小堆，如果发现，然后扫描后面的数据，并与堆顶元素比较，如果比堆顶元素大，那么用该元素替换堆顶，然后再调整为最小堆。最后堆中的元素就是 TOP 10 大）。
- 2.组合归并
- 求出每台电脑上的 TOP 10 后，然后把这 100 台电脑上的 TOP 10 组合起来，共 1000 个数据，再利用上面类似的方法求出 TOP 10 就可以了。

**解法二**：

但如果同一个元素重复出现在不同的电脑中呢，比如拿两台机器求 top 2 的情况来说：

- 第一台的数据分布及各自出现频率为：a(50)，b(50)，c(49)，d(49) ，e(0)，f(0)
- 其中，括号里的数字代表某个数据出现的频率，如 a(50)表示 a 出现了 50 次。
- 第二台的数据分布及各自出现频率为：a(0)，b(0)，c(49)，d(49)，e(50)，f(50)

这个时候，你可以有两种方法：

- 遍历一遍所有数据，重新 hash 取摸，如此使得同一个元素只出现在单独的一台电脑中，然后采用上面所说的方法，统计每台电脑中各个元素的出现次数找出 TOP 10，继而组合 100 台电脑上的 TOP 10，找出最终的 TOP 10。
- 或者，暴力求解：直接统计统计每台电脑中各个元素的出现次数，然后把同一个元素在不同机器中的出现次数相加，最终从所有数据中找出 TOP 10。

**5、有 10 个文件，每个文件 1G，每个文件的每一行存放的都是用户的 query，每个文件的 query 都可能重复。要求你按照 query 的频度排序**

**解法一**：

- 1.hash 映射
- 顺序读取 10 个文件，按照 hash(query)%10 的结果将 query 写入到另外 10 个文件（记为 a0,a1,..a9）中。这样新生成的文件每个的大小大约也 1G（假设 hash 函数是随机的）。
- 2.hash_map 统计
- 找一台内存在 2G 左右的机器，依次对用 hash_map(query, query_count)来统计每个 query 出现的次数。注：hash_map(query, query_count)是用来统计每个 query 的出现次数，不是存储他们的值，出现一次，则 count+1。
- 3.堆/快速/归并排序
- 利用快速/堆/归并排序按照出现次数进行排序，将排序好的 query 和对应的 query_cout 输出到文件中，这样得到了 10 个排好序的文件（记为![](https://ngte-superbed.oss-cn-beijing.aliyuncs.com/book/The-Art-Of-Programming/images/8/8.1/8.1.2.gif)）。最后，对这 10 个文件进行归并排序（内排序与外排序相结合）。

**解法二**：

一般 query 的总量是有限的，只是重复的次数比较多而已，可能对于所有的 query，一次性就可以加入到内存了。这样，我们就可以采用 trie 树/hash_map 等直接来统计每个 query 出现的次数，然后按出现次数做快速/堆/归并排序就可以了。

**解法三**：

与解法 1 类似，但在做完 hash，分成多个文件后，可以交给多个文件来处理，采用分布式的架构来处理（比如 MapReduce），最后再进行合并。

**6、给定 a、b 两个文件，各存放 50 亿个 url，每个 url 各占 64 字节，内存限制是 4G，让你找出 a、b 文件共同的 url？**

**解法**：

可以估计每个文件安的大小为 5G×64=320G，远远大于内存限制的 4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。

- 1.分而治之/hash 映射
- 遍历文件 a，对每个 url 求取![](https://ngte-superbed.oss-cn-beijing.aliyuncs.com/book/The-Art-Of-Programming/images/8/8.1/8.1.3.gif)，然后根据所取得的值将 url 分别存储到 1000 个小文件（记为![](https://ngte-superbed.oss-cn-beijing.aliyuncs.com/book/The-Art-Of-Programming/images/8/8.1/8.1.4.gif)，这里漏写个了 a1）中。这样每个小文件的大约为 300M。遍历文件 b，采取和 a 相同的方式将 url 分别存储到 1000 小文件中（记为![](https://ngte-superbed.oss-cn-beijing.aliyuncs.com/book/The-Art-Of-Programming/images/8/8.1/8.1.5.gif)）。这样处理后，所有可能相同的 url 都在对应的小文件（![](https://ngte-superbed.oss-cn-beijing.aliyuncs.com/book/The-Art-Of-Programming/images/8/8.1/8.1.6.gif)）中，不对应的小文件不可能有相同的 url。然后我们只要求出 1000 对小文件中相同的 url 即可。
- 2.hash_set 统计
- 求每对小文件中相同的 url 时，可以把其中一个小文件的 url 存储到 hash_set 中。然后遍历另一个小文件的每个 url，看其是否在刚才构建的 hash_set 中，如果是，那么就是共同的 url，存到文件里面就可以了。

**7、100 万个数中找出最大的 100 个数**

**解法一**：采用局部淘汰法。选取前 100 个元素，并排序，记为序列 L。然后一次扫描剩余的元素 x，与排好序的 100 个元素中最小的元素比，如果比这个最小的要大，那么把这个最小的元素删除，并把 x 利用插入排序的思想，插入到序列 L 中。依次循环，知道扫描了所有的元素。复杂度为 O(100 万\*100)。

**解法二**：采用快速排序的思想，每次分割之后只考虑比主元大的一部分，直到比主元大的一部分比 100 多的时候，采用传统排序算法排序，取前 100 个。复杂度为 O(100 万\*100)。

**解法三**：在前面的题中，我们已经提到了，用一个含 100 个元素的最小堆完成。复杂度为 O(100 万\*lg100)。

### 举一反三

**1**、怎么在海量数据中找出重复次数最多的一个？

提示：先做 hash，然后求模映射为小文件，求出每个小文件中重复次数最多的一个，并记录重复次数。然后找出上一步求出的数据中重复次数最多的一个就是所求（具体参考前面的题）。

**2**、上千万或上亿数据（有重复），统计其中出现次数最多的前 N 个数据。

提示：上千万或上亿的数据，现在的机器的内存应该能存下。所以考虑采用 hash_map/搜索二叉树/红黑树等来进行统计次数。然后就是取出前 N 个出现次数最多的数据了，可以用第 2 题提到的堆机制完成。

**3**、一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前 10 个词，请给出思想，给出时间复杂度分析。

提示：这题是考虑时间效率。用 trie 树统计每个词出现的次数，时间复杂度是 O(n\*le)（le 表示单词的平准长度）。然后是找出出现最频繁的前 10 个词，可以用堆来实现，前面的题中已经讲到了，时间复杂度是 O(n\*lg10)。所以总的时间复杂度，是 O(n\*le)与 O(n\*lg10)中较大的哪一个。

**4**、1000 万字符串，其中有些是重复的，需要把重复的全部去掉，保留没有重复的字符串。请怎么设计和实现？

提示：这题用 trie 树比较合适，hash_map 也行。当然，也可以先 hash 成小文件分开处理再综合。

**5**、一个文本文件，找出前 10 个经常出现的词，但这次文件比较长，说是上亿行或十亿行，总之无法一次读入内存，问最优解。

提示：首先根据用 hash 并求模，将文件分解为多个小文件，对于单个文件利用上题的方法求出每个文件件中 10 个最常出现的词。然后再进行归并处理，找出最终的 10 个最常出现的词。
